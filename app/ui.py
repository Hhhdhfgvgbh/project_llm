from __future__ import annotations

import time
from pathlib import Path

import psutil
import streamlit as st

from app.config.loader import ConfigLoader
from app.config.schemas import AggregationType, PipelineConfig, StageOutputMode
from app.core.aggregation_engine import AggregationEngine
from app.core.model_registry import ModelRegistry
from app.core.model_wrapper import ModelWrapper
from app.core.pipeline_engine import PipelineEngine
from app.core.resource_manager import ResourceManager
from app.core.session_manager import SessionManager


CONFIG_DIR = Path("config")
MODELS_PATH = CONFIG_DIR / "models.yaml"
PIPELINE_PATH = CONFIG_DIR / "pipeline.yaml"


@st.cache_data(show_spinner=False)
def load_configs() -> tuple[object | None, PipelineConfig | None, list[str]]:
    warnings: list[str] = []

    models_cfg = None
    pipeline_cfg = None

    if MODELS_PATH.exists():
        try:
            models_cfg = ConfigLoader.load_models_config(MODELS_PATH)
        except Exception as exc:  # noqa: BLE001
            warnings.append(f"ĞÑˆĞ¸Ğ±ĞºĞ° models.yaml: {exc}")
    else:
        warnings.append("config/models.yaml Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½")

    if PIPELINE_PATH.exists():
        try:
            pipeline_cfg = ConfigLoader.load_pipeline_config(PIPELINE_PATH)
        except Exception as exc:  # noqa: BLE001
            warnings.append(f"ĞÑˆĞ¸Ğ±ĞºĞ° pipeline.yaml: {exc}")
    else:
        warnings.append("config/pipeline.yaml Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½")

    return models_cfg, pipeline_cfg, warnings


def build_engine(models_cfg: object) -> tuple[PipelineEngine, ModelRegistry]:
    registry = ModelRegistry(models_cfg).build()
    engine = PipelineEngine(
        registry=registry,
        model_wrapper=ModelWrapper(),
        aggregation_engine=AggregationEngine(
            synthesis_callback=lambda model, prompt: f"[SYNTHESIS:{model}]\n{prompt}"
        ),
        resource_manager=ResourceManager(),
        session_manager=SessionManager(),
    )
    overrides = st.session_state.get("strip_reasoning_overrides", {})
    for name, value in overrides.items():
        if name in registry.models:
            registry.models[name].strip_reasoning = bool(value)
    return engine, registry


def build_manual_pipeline_from_ui(models_cfg: object, fallback_pipeline: PipelineConfig | None) -> PipelineConfig:
    available_models = sorted(models_cfg.models.keys())
    if not available_models:
        raise ValueError("ĞĞµÑ‚ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ² models.yaml")

    default_stages = len(fallback_pipeline.base_pipeline.stages) if fallback_pipeline else 2
    stages_count = st.number_input("ĞšĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ ÑÑ‚Ğ°Ğ´Ğ¸Ğ¹", min_value=1, max_value=8, value=default_stages, step=1)

    stage_payloads: list[dict] = []
    stage_ids: list[str] = []

    for idx in range(int(stages_count)):
        st.markdown(f"### Ğ¡Ñ‚Ğ°Ğ´Ğ¸Ñ {idx + 1}")
        stage_id = st.text_input("ID ÑÑ‚Ğ°Ğ´Ğ¸Ğ¸", value=f"manual_stage_{idx + 1}", key=f"m_id_{idx}")
        stage_type = st.selectbox("Ğ¢Ğ¸Ğ¿ ÑÑ‚Ğ°Ğ´Ğ¸Ğ¸", ["single", "multi"], key=f"m_type_{idx}")
        show_prompt_key = f"m_show_prompt_{idx}"
        show_instructions_key = f"m_show_instructions_{idx}"
        st.session_state.setdefault(show_prompt_key, False)
        st.session_state.setdefault(show_instructions_key, False)

        prompt_btn_label = "â– Ğ¡ĞºÑ€Ñ‹Ñ‚ÑŒ System prompt" if st.session_state[show_prompt_key] else "â• Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ System prompt"
        instructions_btn_label = (
            "â– Ğ¡ĞºÑ€Ñ‹Ñ‚ÑŒ Instructions" if st.session_state[show_instructions_key] else "â• Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Instructions"
        )

        btn_col1, btn_col2 = st.columns(2)
        with btn_col1:
            if st.button(prompt_btn_label, key=f"m_prompt_btn_{idx}", use_container_width=True):
                st.session_state[show_prompt_key] = not st.session_state[show_prompt_key]
        with btn_col2:
            if st.button(instructions_btn_label, key=f"m_instructions_btn_{idx}", use_container_width=True):
                st.session_state[show_instructions_key] = not st.session_state[show_instructions_key]

        system_prompt = ""
        if st.session_state[show_prompt_key]:
            system_prompt = st.text_area("System prompt", key=f"m_prompt_{idx}", height=80)
        instructions = ""
        if st.session_state[show_instructions_key]:
            instructions = st.text_area("Instructions (Ğ½ĞµĞ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾)", key=f"m_instructions_{idx}", height=80)

        col1, col2, col3 = st.columns(3)
        with col1:
            temperature = st.number_input("temperature", min_value=0.0, max_value=2.0, value=0.7, step=0.05, key=f"m_t_{idx}")
            top_p = st.number_input("top_p", min_value=0.0, max_value=1.0, value=0.95, step=0.05, key=f"m_p_{idx}")
        with col2:
            max_tokens = st.number_input("max_tokens", min_value=1, max_value=8192, value=1024, step=32, key=f"m_mt_{idx}")
            n_ctx = st.number_input("n_ctx", min_value=256, max_value=65536, value=8192, step=256, key=f"m_ctx_{idx}")
        with col3:
            n_gpu_layers = st.number_input("n_gpu_layers", min_value=-1, max_value=512, value=0, step=1, key=f"m_gpu_{idx}")
            threads = st.number_input("threads", min_value=1, max_value=128, value=8, step=1, key=f"m_th_{idx}")

        generation = {
            "temperature": float(temperature),
            "top_p": float(top_p),
            "max_tokens": int(max_tokens),
            "n_ctx": int(n_ctx),
            "n_gpu_layers": int(n_gpu_layers),
            "threads": int(threads),
        }

        output_mode = st.selectbox(
            "Ğ§Ñ‚Ğ¾ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ğ² ÑĞ»ĞµĞ´ÑƒÑÑ‰ÑƒÑ ÑÑ‚Ğ°Ğ´Ğ¸Ñ",
            options=[item.value for item in StageOutputMode],
            format_func=lambda value: "Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚" if value == StageOutputMode.ANSWER_ONLY.value else "Ğ’Ğ¾Ğ¿Ñ€Ğ¾Ñ + Ğ¾Ñ‚Ğ²ĞµÑ‚",
            key=f"m_output_mode_{idx}",
        )

        stage_data: dict = {
            "id": stage_id.strip() or f"manual_stage_{idx + 1}",
            "type": stage_type,
            "system_prompt": system_prompt,
            "instructions": instructions,
            "output_mode": output_mode,
            "generation": generation,
        }

        if idx > 0:
            input_count = st.number_input(
                "Ğ¡ĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ²Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ğ¸Ñ‚ÑŒ",
                min_value=1,
                max_value=len(stage_ids),
                value=1,
                step=1,
                key=f"m_input_count_{idx}",
            )
            selected_inputs: list[str] = []
            for input_idx in range(int(input_count)):
                selected = st.selectbox(
                    f"Ğ’Ñ‹Ñ…Ğ¾Ğ´ Ğ±ĞµÑ€Ñ‘Ğ¼ Ğ¸Ğ· #{input_idx + 1}",
                    options=stage_ids,
                    index=min(input_idx, len(stage_ids) - 1),
                    key=f"m_input_{idx}_{input_idx}",
                )
                selected_inputs.append(selected)

            deduped_inputs = list(dict.fromkeys(selected_inputs))
            stage_data["input_from"] = deduped_inputs[0] if len(deduped_inputs) == 1 else deduped_inputs

        if stage_type == "single":
            stage_data["model"] = st.selectbox("ĞœĞ¾Ğ´ĞµĞ»ÑŒ", options=available_models, key=f"m_model_{idx}")
        else:
            selected = st.multiselect(
                "ĞœĞ¾Ğ´ĞµĞ»Ğ¸",
                options=available_models,
                default=available_models[:2],
                key=f"m_models_{idx}",
            )
            stage_data["models"] = selected

            agg_type = st.selectbox(
                "Aggregation",
                options=[item.value for item in AggregationType],
                key=f"m_agg_type_{idx}",
            )
            agg_payload = {"type": agg_type}
            if agg_type in (AggregationType.SYNTHESIS.value, AggregationType.CUSTOM_TEMPLATE.value):
                agg_payload["synthesis_model"] = st.selectbox(
                    "Synthesis model",
                    options=available_models,
                    key=f"m_agg_model_{idx}",
                )
            if agg_type == AggregationType.CUSTOM_TEMPLATE.value:
                agg_payload["template"] = st.text_area(
                    "Template",
                    value="A={{model_1}}\n\nB={{model_2}}",
                    key=f"m_agg_tpl_{idx}",
                )
            stage_data["aggregation"] = agg_payload

        stage_payloads.append(stage_data)
        stage_ids.append(stage_data["id"])

    payload = {
        "version": 1,
        "base_pipeline": {
            "execution_mode": "sequential",
            "stages": stage_payloads,
        },
    }
    return PipelineConfig.model_validate(payload)


def render_sidebar(models_cfg: object | None, pipeline_cfg: PipelineConfig | None, warnings: list[str]) -> None:
    with st.sidebar:
        st.title("âš™ï¸ ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸")

        if st.button("ğŸ”„ ĞŸĞµÑ€ĞµĞ·Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³Ğ¸"):
            st.cache_data.clear()
            st.rerun()

        if models_cfg is not None:
            st.success(f"âœ… ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Ğ² ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³Ğµ: {len(models_cfg.models)}")
        else:
            st.error("âŒ models.yaml Ğ½Ğµ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½")

        if pipeline_cfg is not None:
            st.success(f"âœ… Ğ¡Ñ‚Ğ°Ğ´Ğ¸Ğ¹ Ğ² base pipeline: {len(pipeline_cfg.base_pipeline.stages)}")
        else:
            st.warning("âš ï¸ pipeline.yaml Ğ½Ğµ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½")

        for warning in warnings:
            st.warning(f"âš ï¸ {warning}")

        st.divider()
        ram_gb = psutil.virtual_memory().available / (1024**3)
        st.metric("Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ RAM", f"{ram_gb:.1f} GB")

        vram_gb = st.number_input("ĞÑ†ĞµĞ½ĞºĞ° Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾Ğ¹ VRAM (GB)", min_value=0.0, value=8.0, step=0.5)
        safety_ok = ResourceManager.check_safety_coefficients(ram_gb, vram_gb)
        st.success("âœ… Ğ ĞµÑÑƒÑ€ÑÑ‹ Ğ² Ğ½Ğ¾Ñ€Ğ¼Ğµ") if safety_ok else st.error("âŒ ĞĞµĞ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ñ€ĞµÑÑƒÑ€ÑĞ¾Ğ²")


def render_base_tab(models_cfg: object | None, pipeline_cfg: PipelineConfig | None) -> None:
    st.header("Ğ‘Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼ (Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ pipeline.yaml)")

    selected_pipeline_name = "base_pipeline"
    if pipeline_cfg is not None:
        pipeline_names = pipeline_cfg.list_pipelines()
        selected_pipeline_name = st.selectbox(
            "ĞŸĞ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½",
            options=pipeline_names,
            format_func=lambda value: "base_pipeline (Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ)" if value == "base_pipeline" else value,
        )

    text_input = st.text_area("Ğ’Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ñ‚ĞµĞºÑÑ‚", height=200, placeholder="Ğ’ÑÑ‚Ğ°Ğ²ÑŒÑ‚Ğµ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ Ğ¸Ğ»Ğ¸ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚...")

    run_base = st.button("â–¶ï¸ Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½", type="primary", use_container_width=True)

    if not run_base:
        return

    if not text_input.strip():
        st.warning("Ğ’Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸")
        return

    if models_cfg is None or pipeline_cfg is None:
        st.error("ĞĞµĞ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ·Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ: Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³Ğ¸")
        return

    with st.spinner("ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¸ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½Ğ°..."):
        try:
            engine, registry = build_engine(models_cfg)
            if registry.base_mode_blocked:
                st.error("Base mode Ğ·Ğ°Ğ±Ğ»Ğ¾ĞºĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½: Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ Ğ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸")
                return

            selected_pipeline = pipeline_cfg.get_pipeline(selected_pipeline_name)
            run_pipeline = pipeline_cfg.model_copy(update={"base_pipeline": selected_pipeline})

            start = time.perf_counter()
            result = engine.run(run_pipeline, user_input=text_input)
            elapsed = time.perf_counter() - start
            st.success("Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¾")
            st.metric("Ğ’Ñ€ĞµĞ¼Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°", f"{elapsed:.2f} ÑĞµĞº")

            st.subheader("ĞŸÑ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹")
            for step in result.steps:
                with st.expander(f"Ğ­Ñ‚Ğ°Ğ¿ {step.stage_id} ({step.stage_type})"):
                    for model_name, output in step.model_outputs.items():
                        st.markdown(f"**{model_name}**")
                        st.code(output[:1500])
                    st.markdown("**Aggregated output**")
                    st.write(step.aggregated_output)

            st.subheader("Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚")
            st.write(result.final_output)
            st.download_button(
                "Ğ¡ĞºĞ°Ñ‡Ğ°Ñ‚ÑŒ final.txt",
                result.final_output,
                file_name="final.txt",
                mime="text/plain",
            )
        except Exception as exc:  # noqa: BLE001
            st.exception(exc)


def render_manual_tab(models_cfg: object | None, pipeline_cfg: PipelineConfig | None) -> None:
    st.header("Ğ ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼")
    st.info("Ğ’Ğ¸Ğ·ÑƒĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ĞºĞ¾Ğ½ÑÑ‚Ñ€ÑƒĞºÑ‚Ğ¾Ñ€: Ğ½Ğ°ÑÑ‚Ñ€Ğ°Ğ¸Ğ²Ğ°Ğ¹Ñ‚Ğµ ÑÑ‚Ğ°Ğ´Ğ¸Ğ¸, Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸, Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸, Ğ°Ğ³Ñ€ĞµĞ³Ğ°Ñ†Ğ¸Ñ Ğ¸ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ğ²Ñ…Ğ¾Ğ´Ğ¾Ğ² Ğ¸Ğ· Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¸Ñ… ÑÑ‚Ğ°Ğ¿Ğ¾Ğ².")

    text_input = st.text_area("Ğ’Ñ…Ğ¾Ğ´ Ğ´Ğ»Ñ manual Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°", key="manual_input", height=160)

    if models_cfg is None:
        st.error("ĞĞµĞ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ·Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ manual mode: Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ñ‹Ğ¹ models.yaml")
        return

    try:
        manual_pipeline = build_manual_pipeline_from_ui(models_cfg, pipeline_cfg)
        with st.expander("ĞŸÑ€ĞµĞ´Ğ¿Ñ€Ğ¾ÑĞ¼Ğ¾Ñ‚Ñ€ pipeline (JSON)"):
            st.json(manual_pipeline.model_dump())
    except Exception as exc:  # noqa: BLE001
        st.error("ĞÑˆĞ¸Ğ±ĞºĞ° Ğ² ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ manual pipeline")
        st.exception(exc)
        return

    run_manual = st.button("â–¶ï¸ Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚ÑŒ Ğ² manual Ñ€ĞµĞ¶Ğ¸Ğ¼Ğµ", use_container_width=True)
    if not run_manual:
        return

    if not text_input.strip():
        st.warning("Ğ’Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ»Ñ manual Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°")
        return

    with st.spinner("Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ manual Ñ€ĞµĞ¶Ğ¸Ğ¼Ğ°..."):
        try:
            engine, _ = build_engine(models_cfg)
            start = time.perf_counter()
            result = engine.run(manual_pipeline, user_input=text_input)
            elapsed = time.perf_counter() - start
            st.success("Manual run Ğ·Ğ°Ğ²ĞµÑ€ÑˆÑ‘Ğ½")
            st.metric("Ğ’Ñ€ĞµĞ¼Ñ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°", f"{elapsed:.2f} ÑĞµĞº")

            st.subheader("ĞŸÑ€Ğ¾Ğ¼ĞµĞ¶ÑƒÑ‚Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹")
            for step in result.steps:
                with st.expander(f"Ğ­Ñ‚Ğ°Ğ¿ {step.stage_id} ({step.stage_type})"):
                    for model_name, output in step.model_outputs.items():
                        st.markdown(f"**{model_name}**")
                        st.code(output[:1500])
                    st.markdown("**Aggregated output**")
                    st.write(step.aggregated_output)

            st.subheader("Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚")
            st.write(result.final_output)
        except Exception as exc:  # noqa: BLE001
            st.exception(exc)


def render_history_tab() -> None:
    st.header("Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ ÑĞµÑÑĞ¸Ğ¹")
    manager = SessionManager()
    sessions = manager.list_sessions()

    if not sessions:
        st.info("ĞŸĞ¾ĞºĞ° Ğ½ĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ñ‘Ğ½Ğ½Ñ‹Ñ… ÑĞµÑÑĞ¸Ğ¹")
        return

    for item in sessions:
        with st.expander(f"{item.id} | mode={item.mode} | {item.created_at}"):
            st.write(f"Ğ­Ñ‚Ğ°Ğ¿Ğ¾Ğ²: {len(item.stages)}")
            if st.button("ĞÑ‚ĞºÑ€Ñ‹Ñ‚ÑŒ", key=f"open_{item.id}"):
                details = manager.load_session(item.id)
                st.subheader("Final output")
                st.write(details.final_output or "(empty)")
                st.subheader("Stage artifacts")
                st.json(details.stages)


def render_models_tab(models_cfg: object | None) -> None:
    st.header("Ğ¡Ñ‚Ğ°Ñ‚ÑƒÑ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹")
    if models_cfg is None:
        st.error("models.yaml Ğ½Ğµ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½")
        return

    if "strip_reasoning_overrides" not in st.session_state:
        st.session_state["strip_reasoning_overrides"] = {}

    _, registry = build_engine(models_cfg)
    overrides: dict[str, bool] = st.session_state["strip_reasoning_overrides"]

    st.caption("ĞŸĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ°Ñ‚ĞµĞ»Ğ¸ Ğ½Ğ¸Ğ¶Ğµ Ğ¼ĞµĞ½ÑÑÑ‚ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºÑƒ Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¹ ÑĞµÑÑĞ¸Ğ¸ UI.")
    for model in registry.list_models():
        overrides.setdefault(model.name, model.strip_reasoning)
        col1, col2, col3 = st.columns([3, 2, 2])
        with col1:
            st.write(f"**{model.name}** â€” {model.path}")
            st.caption(model.description or "ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ Ğ½Ğµ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ¾")
        with col2:
            st.write(f"quant: {model.quantization} | ctx: {model.generation.n_ctx}")
        with col3:
            toggled = st.toggle(
                "ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° Ñ€Ğ°ÑÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğ¹",
                value=overrides[model.name],
                key=f"strip_reasoning_toggle_{model.name}",
            )
            overrides[model.name] = toggled

    if registry.warnings:
        st.warning("ĞĞµĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹:")
        for warning in registry.warnings:
            st.write(f"- {warning}")


def main() -> None:
    st.set_page_config(
        page_title="Project LLM Orchestrator",
        page_icon="ğŸ§ ",
        layout="wide",
        initial_sidebar_state="expanded",
    )

    st.title("ğŸ§  Project LLM Orchestrator")
    models_cfg, pipeline_cfg, warnings = load_configs()
    render_sidebar(models_cfg, pipeline_cfg, warnings)

    tab_base, tab_manual, tab_history, tab_models = st.tabs(
        ["ğŸ›  Ğ‘Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼", "ğŸ§© Ğ ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ñ€ĞµĞ¶Ğ¸Ğ¼", "ğŸ“– Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ", "ğŸ“Š ĞœĞ¾Ğ´ĞµĞ»Ğ¸"]
    )

    with tab_base:
        render_base_tab(models_cfg, pipeline_cfg)

    with tab_manual:
        render_manual_tab(models_cfg, pipeline_cfg)

    with tab_history:
        render_history_tab()

    with tab_models:
        render_models_tab(models_cfg)


if __name__ == "__main__":
    main()
