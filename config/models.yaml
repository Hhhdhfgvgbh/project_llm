version: 1
models_directory: "./models"
defaults:
  n_ctx: 8192
  n_gpu_layers: 0
  temperature: 0.7
  top_p: 0.95
  max_tokens: 1024
  threads: 8
models:
  phi:
    file: "Phi-3.5-mini-instruct-Q5_K_M.gguf"
    description: "Phi-3.5-mini-instruct-Q5_K_M"
    quantization: "Q5_K_M"
    required_for_base: true
    strip_reasoning: true
    overrides:
      n_ctx: 16384
      n_gpu_layers: 35
  deepseek:
    file: "DeepSeek-R1-Distill-Llama-8B-Q5_K_M.gguf"
    description: "DeepSeek-R1-Distill-Llama-8B-Q5_K_M"
    quantization: "Q5_K_M"
    required_for_base: true
    strip_reasoning: true
    overrides:
      n_ctx: 16384
      n_gpu_layers: 35
  gemma_2:
    file: "gemma-2-9b-it-Q5_K_M.gguf"
    description: "gemma-2-9b-it-Q5_K_M"
    quantization: "Q5_K_M"
    required_for_base: true
    strip_reasoning: true
    overrides:
      n_ctx: 16384
      n_gpu_layers: 35
  essential_rnj:
    file: "EssentialAI_rnj-1-instruct-Q5_K_M.gguf"
    description: "не работает?"
    quantization: "Q5_K_M"
    required_for_base: true
    strip_reasoning: true
    overrides:
      n_ctx: 16384
      n_gpu_layers: 35
  gemma_3:
    file: "google_gemma-3-12b-it-Q5_K_S.gguf"
    description: "Гемма 3 8ГБ"
    quantization: "Q5_K_M"
    required_for_base: true
    strip_reasoning: true
    overrides:
      n_ctx: 16384
      n_gpu_layers: 35
